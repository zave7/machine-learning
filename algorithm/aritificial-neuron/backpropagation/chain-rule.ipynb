{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x = 5\n",
      "z = 3\n",
      "y = 18\n",
      "Function2 dx = 12.000000000025324\n",
      "Function1 dx = 0.9999999999976694\n",
      "Chain Rule dx = 11.999999999997357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.999999999997357"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 1e-4 # 스텝\n",
    "get_derivative = lambda f, x: (f(x+h) - f(x-h)) / (2*h) # 중앙 차분\n",
    "\n",
    "class Function1:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.function = lambda x: x - 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = self.function(x)\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dy_dx):\n",
    "        self.dx = get_derivative(f=self.function, x=self.x)\n",
    "        self.dout = self.dx * dy_dx\n",
    "        return self.dout\n",
    "\n",
    "class Function2:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.function = lambda z: 2 * z**2\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        y = self.function(z)\n",
    "        return y\n",
    "    \n",
    "    def backward(self):\n",
    "        self.dx = get_derivative(f=self.function, x=self.z)\n",
    "        self.dout = self.dx\n",
    "        return self.dout\n",
    "        \n",
    "class Function:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.function_1 = Function1()\n",
    "        self.function_2 = Function2()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input x = {x}\")\n",
    "        z = self.function_1.forward(x)\n",
    "        print(f\"z = {z}\")\n",
    "        y = self.function_2.forward(z)\n",
    "        print(f\"y = {y}\")\n",
    "        return y\n",
    "\n",
    "    def backward(self):\n",
    "        f2_dout = self.function_2.backward()\n",
    "        f1_dout = self.function_1.backward(f2_dout)\n",
    "        print(f\"Function2 dx = {self.function_2.dx}\")\n",
    "        print(f\"Function1 dx = {self.function_1.dx}\")\n",
    "        print(f\"Chain Rule dx = {f1_dout}\")\n",
    "        return f1_dout\n",
    "\n",
    "function = Function()\n",
    "function.forward(x=5)\n",
    "function.backward()\n",
    "\n",
    "# 전방 차분 결과 값\n",
    "# Function2 dx = 12.000200000024108\n",
    "# Function1 dx = 0.9999999999976694\n",
    "# Chain Rule dx = 12.000199999996141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Function:\n",
    "\n",
    "    def __init__(self, function, d_function):\n",
    "        self.function = function\n",
    "        self.d_function = d_function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = self.function(x)\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dy_dx=None):\n",
    "        self.dx = self.d_function(self.x)\n",
    "        if dy_dx is not None:\n",
    "            self.dout = self.dx * dy_dx # 먼저 X를 전치하고 출력 미분과 행렬곱 해주면 된다.\n",
    "        else:\n",
    "            self.dout = self.dx\n",
    "        return self.dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid forward = [0.95257413 0.99966465]\n",
      "Sigmoid backward = (1, array([-0.04978707, -0.00033546]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "h = 1e-4\n",
    "get_derivative = lambda f, x: (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.f1 = { \"f\": lambda x: -1 * x }\n",
    "        self.f2 = { \"f\": lambda x: np.exp(x) }\n",
    "        self.f3 = { \"f\": lambda x: 1 + x }\n",
    "        self.f4 = { \"f\": lambda x: 1 / x }\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.f1[\"f\"](x)\n",
    "        self.f1[\"x\"] = x\n",
    "        z2 = self.f2[\"f\"](z1)\n",
    "        self.f2[\"x\"] = z1\n",
    "        z3 = self.f3[\"f\"](z2)\n",
    "        self.f3[\"x\"] = z2\n",
    "        y = self.f4[\"f\"](z3)\n",
    "        self.f4[\"x\"] = z3\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout, dw=None):\n",
    "        self.f4[\"dx\"] = get_derivative(f=self.f4[\"f\"], x=self.f4[\"x\"]) * dw if dw is not None else dout\n",
    "        self.f3[\"dx\"] = get_derivative(f=self.f3[\"f\"], x=self.f3[\"x\"]) * self.f4[\"dx\"]\n",
    "        self.f2[\"dx\"] = get_derivative(f=self.f2[\"f\"], x=self.f2[\"x\"]) * self.f3[\"dx\"]\n",
    "        self.f1[\"dx\"] = get_derivative(f=self.f1[\"f\"], x=self.f1[\"x\"]) * self.f2[\"dx\"]\n",
    "        return dout, self.f1[\"dx\"]\n",
    "    \n",
    "sigmoid = Sigmoid()\n",
    "forward = sigmoid.forward(np.array([3, 8]))\n",
    "print(f\"Sigmoid forward = {forward}\") # 0.9525741268224334\n",
    "\n",
    "backward = sigmoid.backward(1)\n",
    "print(f\"Sigmoid backward = {backward}\") # 0.04517666021639175\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Affine:\n",
    "    \n",
    "    def __init__(self, shape, lr):\n",
    "        self.w = np.random.randn(*shape) # wait 은 앞에꺼, b 는 뒤에꺼\n",
    "        self.b = np.random.randn()\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        print(f\"dot {self.x @ self.w}\")\n",
    "        return self.x @ self.w + self.b\n",
    "    \n",
    "    def backward(self, dout, dw):\n",
    "        print(f\"dw {dw.shape}\")\n",
    "        self.dout_w = np.transpose(self.x) @ dw\n",
    "        print(f\"self.dout_w = {self.dout_w.shape}\")\n",
    "        self.w -= self.lr * self.dout_w\n",
    "        self.b -= self.lr * dout\n",
    "        return dout, self.dout_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_0_9 = [[6.90775528]\n",
      " [6.2146081 ]\n",
      " [5.80914299]\n",
      " [5.52146092]]\n",
      "[[47.7887462 ]\n",
      " [38.73750175]\n",
      " [33.89855864]\n",
      " [30.67034821]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from functools import reduce\n",
    "\n",
    "class BCELossFunction:\n",
    "    \n",
    "    def __init__(self, y):\n",
    "        \n",
    "        # - [ ylog(yhat) + (1-y)log(1-yhat) ]\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        f0 = Function(function=lambda x: np.log(x),\n",
    "                        d_function=lambda x: 1 / x)\n",
    "        f1 = Function(function=lambda y_pred: y_pred[0] * y_pred[1],\n",
    "                        d_function=lambda y: y[1]) # 변하는 값인 예측값에 대해 미분\n",
    "        f2 = Function(function=lambda x: (1 - x),\n",
    "                        d_function=lambda x: np.full(x.shape, -1))\n",
    "        f3 = Function(function=lambda a_b: a_b[0] + a_b[1],\n",
    "                        d_function=lambda a_b: a_b[0] + a_b[1])\n",
    "        f4 = Function(function=lambda x: -x,\n",
    "                        d_function=lambda x: np.full(x.shape, -1))\n",
    "        \n",
    "        self.f_list = [\n",
    "            copy.deepcopy(f0),\n",
    "            copy.deepcopy(f1),\n",
    "            copy.deepcopy(f2),\n",
    "            copy.deepcopy(f0),\n",
    "            copy.deepcopy(f2),\n",
    "            copy.deepcopy(f1),\n",
    "            copy.deepcopy(f3),\n",
    "            copy.deepcopy(f4)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, pred):\n",
    "        step_1 = self.f_list[0].forward(pred)\n",
    "        step_2 = self.f_list[1].forward((self.y, step_1)) # y가 1일때\n",
    "        step_3 = self.f_list[2].forward(pred)\n",
    "        step_4 = self.f_list[3].forward(step_3)\n",
    "        step_5 = self.f_list[4].forward(self.y)\n",
    "        step_6 = self.f_list[5].forward((step_5, step_4)) # y가 0일때\n",
    "        step_7 = self.f_list[6].forward((step_2, step_6))\n",
    "        loss_value = self.f_list[7].forward(step_7)\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self):\n",
    "        dout = reduce(lambda acc, f: f.backward(acc), \n",
    "                        self.f_list[::-1], \n",
    "                        None)\n",
    "        return dout, dout\n",
    "        \n",
    "bce_loss = BCELossFunction(np.array([1, 1, 1, 1]))\n",
    "_0_001 = bce_loss.forward(np.array([0.001, 0.002, 0.003, 0.004]).reshape(-1, 1))\n",
    "print(f\"_0_9 = {_0_001}\")\n",
    "print(bce_loss.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuron:\n",
    "    \n",
    "    def __init__(self, shape, lr):\n",
    "        self.affine = Affine(shape=shape, lr=lr)\n",
    "        self.activation = Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.affine.forward(x)\n",
    "        a = self.activation.forward(z)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, dout, dw):\n",
    "        dout, dw = self.activation.backward(dout, dw)\n",
    "        return self.affine.backward(dout, dw)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, shape, lr):\n",
    "        # 인공 뉴런 리스트\n",
    "        self.neurons = ArtificialNeuron(shape=shape, lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.neurons.forward(x)\n",
    "    \n",
    "    def backward(self, dout, dw):\n",
    "        return self.neurons.backward(dout, dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot [[ 0.          0.        ]\n",
      " [ 0.82271076 -0.14138542]\n",
      " [ 0.09969753 -1.2096325 ]\n",
      " [ 0.92240829 -1.35101792]]\n",
      "dot [[0.6789993 ]\n",
      " [0.98674037]\n",
      " [0.70245987]\n",
      " [1.00854988]]\n",
      "loss_value = [[2.92138768]\n",
      " [0.04099389]\n",
      " [0.05411393]\n",
      " [3.23570173]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ArtificialNeuron.backward() missing 1 required positional argument: 'dw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         reduce(\u001b[39mlambda\u001b[39;00m out, layer: layer\u001b[39m.\u001b[39mbackward(dout\u001b[39m=\u001b[39mout) \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m layer\u001b[39m.\u001b[39mbackward(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m xor_ann \u001b[39m=\u001b[39m XORNetwork(neuron_count_list\u001b[39m=\u001b[39m[\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m], lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, criterion_value\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m xor_ann\u001b[39m.\u001b[39;49mfit([[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m],[\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m],[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]], [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(loss_value \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion_value): \u001b[39m# 학습 종료 기준 값\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward()\n",
      "\u001b[1;32m/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     reduce(\u001b[39mlambda\u001b[39;49;00m out, layer: layer\u001b[39m.\u001b[39;49mbackward(dout\u001b[39m=\u001b[39;49mout) \u001b[39mif\u001b[39;49;00m out \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m layer\u001b[39m.\u001b[39;49mbackward(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     reduce(\u001b[39mlambda\u001b[39;00m out, layer: layer\u001b[39m.\u001b[39;49mbackward(dout\u001b[39m=\u001b[39;49mout) \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m layer\u001b[39m.\u001b[39mbackward(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;32m/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dout):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zave/Private/dev/learning/ai/machine-learning/algorithm/aritificial-neuron/backpropagation/chain-rule.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneurons\u001b[39m.\u001b[39;49mbackward(dout)\n",
      "\u001b[0;31mTypeError\u001b[0m: ArtificialNeuron.backward() missing 1 required positional argument: 'dw'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "class XORNetwork():\n",
    "    \n",
    "    def __init__(self, neuron_count_list, lr, criterion_value):\n",
    "        self.lr = lr\n",
    "        self.criterion_value = criterion_value\n",
    "        self.neuron_count_list = neuron_count_list\n",
    "        \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        self._set_neuron(x.shape[1], y) # Layer, Neuron 초기화\n",
    "        loss_value = None # 손실함수 값\n",
    "        for _ in range(1_000): # 최대 1000 번 학습\n",
    "            loss_value = self._forward(x) # 순전파\n",
    "            print(f\"loss_value = {loss_value}\")\n",
    "            if np.all(loss_value < self.criterion_value): # 학습 종료 기준 값\n",
    "                break\n",
    "            self._backward() # 역전파\n",
    "            \n",
    "    def _set_neuron(self, x_size, y):\n",
    "        self.layers = [] # Layer\n",
    "        front_size = x_size\n",
    "        if self.neuron_count_list == 1: # Layer 가 1개일 경우 shape\n",
    "            self.layers = [ Layer(shape=(front_size, 1), lr=self.lr) ]\n",
    "        else:\n",
    "            self.layers = []\n",
    "            for count in self.neuron_count_list: # Layer 별 Affine Shape 지정\n",
    "                self.layers.append(Layer(shape=(front_size, count), lr=self.lr))\n",
    "                front_size = count\n",
    "        self.layers.append(BCELossFunction(y))\n",
    "            \n",
    "    def _forward(self, x) -> float:\n",
    "        return reduce(lambda out, layer: layer.forward(out), self.layers, x) # 출력값 저장하고 다음으로 계속 넘겨주기 마지막은 손실함수\n",
    "\n",
    "    def _backward(self) -> float:\n",
    "        reduce(lambda out, layer: layer.backward(dout=out[0], dw=out[1]) if out is not None else layer.backward(), self.layers[::-1], None)\n",
    "\n",
    "xor_ann = XORNetwork(neuron_count_list=[2, 1], lr=0.01, criterion_value=0.01)\n",
    "xor_ann.fit([[0, 0],[0, 1],[1, 0],[1, 1]], [0, 1, 1, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
